# Name of your application. Used to uniquely configure containers.
service: ilv_store

# Name of the container image.
image: silmardll/ilv_store

# Deploy to these servers.
servers:
  web:
    - <%= ENV.fetch("SERVER_IP", "192.168.0.1") %>
  # job:
  #   hosts:
  #     - 192.168.0.1
  #   cmd: bin/jobs

# Enable SSL auto certification via Let's Encrypt and allow for multiple apps on a single web server.
# Remove this section when using multiple web servers and ensure you terminate SSL at your load balancer.
#
# Note: If using Cloudflare, set encryption mode in SSL/TLS setting to "Full" to enable CF-to-app encryption.
proxy:
  ssl: true
  host: tienda.ilvmx.org

# Credentials for your image host.
registry:
  # Specify the registry server, if you're not using Docker Hub
  # server: registry.digitalocean.com / ghcr.io / ...
  username: silmardll

  # Always use an access token rather than real password when possible.
  password:
    - KAMAL_REGISTRY_PASSWORD

# Inject ENV variables into containers (secrets come from .kamal/secrets).
env:
  secret:
    - RAILS_MASTER_KEY
    - POSTGRES_PASSWORD
    - UNI_ONE_API_KEY
    - SENTRY_DSN
    - SENTRY_ENABLED_ENVIRONMENTS
    - CLOUDFLARE_R2_JURISDICTION_ENDPOINT
    - CLOUDFLARE_R2_ACCESS_KEY_ID
    - CLOUDFLARE_R2_SECRET_ACCESS_KEY
    - CLOUDFLARE_R2_BUCKET_NAME
    - STRIPE_PUBLISH_KEY
    - STRIPE_SECRET_KEY
    - STRIPE_WEBHOOK_SECRET
    - TYPESENSE_API_KEY
    - SALES_EMAIL_ADDRESS
  clear:
    # Run the Solid Queue Supervisor inside the web server's Puma process to do jobs.
    # When you start using multiple servers, you should split out job processing to a dedicated machine.
    SOLID_QUEUE_IN_PUMA: true

    # Set number of processes dedicated to Solid Queue (default: 1)
    # JOB_CONCURRENCY: 3

    # Set number of cores available to the application on each server (default: 1).
    # WEB_CONCURRENCY: 2

    # Match this to any external database server to configure Active Record correctly
    # Use ilv_store-db for a db accessory server on same machine via local kamal docker network.
    DB_HOST: ilv_store-db

    # Typesense configuration
    TYPESENSE_HOST: ilv_store-typesense
    TYPESENSE_PORT: 8108
    TYPESENSE_PROTOCOL: http

    # Log everything from Rails
    RAILS_LOG_LEVEL: debug

# Aliases are triggered with "bin/kamal <alias>". You can overwrite arguments on invocation:
# "bin/kamal logs -r job" will tail logs from the first server in the job section.
aliases:
  console: app exec --interactive --reuse "bin/rails console"
  shell: app exec --interactive --reuse "bash"
  logs: app logs -f
  dbc: app exec --interactive --reuse "bin/rails dbconsole"
  typesense-reindex: app exec "bin/rails typesense:recreate"


# Use a persistent storage volume for sqlite database files and local Active Storage files.
# Recommended to change this to a mounted volume path that is backed up off server.
volumes:
  - "ilv_store_storage:/rails/storage"


# Bridge fingerprinted assets, like JS and CSS, between versions to avoid
# hitting 404 on in-flight requests. Combines all files from new and old
# version inside the asset_path.
asset_path: /rails/public/assets

# Configure the image builder.
builder:
  arch: amd64

  # # Build image via remote server (useful for faster amd64 builds on arm64 computers)
  # remote: ssh://docker@docker-builder-server
  #
  # # Pass arguments and secrets to the Docker build process
  # args:
  #   RUBY_VERSION: ruby-3.4.1
  # secrets:
  #   - GITHUB_TOKEN
  #   - RAILS_MASTER_KEY

# Use a different ssh user than root
ssh:
  port: 20202
#   user: app

accessories:
  db:
    image: postgres:16
    host:  <%= ENV.fetch("SERVER_IP", "192.168.0.1") %>
    port: "127.0.0.1:5432:5432"
    env:
      clear:
        POSTGRES_USER: rails
        POSTGRES_DB: postgres
      secret:
        - POSTGRES_PASSWORD
    files:
     # - config/mysql/production.cnf:/etc/mysql/my.cnf
      - db/production.sql:/docker-entrypoint-initdb.d/setup.sql
    directories:
      - data:/var/lib/postgresql/data

  typesense:
    image: typesense/typesense:26.0
    host: <%= ENV.fetch("SERVER_IP", "192.168.0.1") %>
    port: "127.0.0.1:8108:8108"
    env:
      clear:
        TYPESENSE_DATA_DIR: /data
      secret:
        - TYPESENSE_API_KEY
    directories:
      - typesense-data:/data
    cmd: --enable-cors
    options:
      health-cmd: "bash -c 'exec 3<>/dev/tcp/127.0.0.1/8108' || exit 1"
      health-interval: "10s"
      health-timeout: "5s"
      health-retries: "6"
      health-start-period: "60s"
# Use accessory services (secrets come from .kamal/secrets).
# accessories:
#   db:
#     image: mysql:8.0
#     host: 192.168.0.2
#     # Change to 3306 to expose port to the world instead of just local network.
#     port: "127.0.0.1:3306:3306"
#     env:
#       clear:
#         MYSQL_ROOT_HOST: '%'
#       secret:
#         - MYSQL_ROOT_PASSWORD
#     files:
#       - config/mysql/production.cnf:/etc/mysql/my.cnf
#       - db/production.sql:/docker-entrypoint-initdb.d/setup.sql
#     directories:
#       - data:/var/lib/mysql
#   redis:
#     image: redis:7.0
#     host: 192.168.0.2
#     port: 6379
#     directories:
#       - data:/data
